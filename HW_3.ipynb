{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import collections\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('preprocessing.pickle', 'rb') as f:\n",
    "    combine_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape[0] + test_df.shape[0] == combine_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thank, lyft, credit, use, cause, offer, wheel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0   @user when a father is dysfunctional and is s...   \n",
       "1   2    0.0  @user @user thanks for #lyft credit i can't us...   \n",
       "\n",
       "                                         clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thank, lyft, credit, use, cause, offer, wheel...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тема “Создание признакового пространства”\n",
    "\n",
    "Используем предобработанные в рамках 1-ого домашнего задания датасет combine_df_prepocessed.pkl. Используем столбец 'clean_tweet'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание 1.  \n",
    "Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_combine = combine_df.clean_tweet.to_string()\n",
    "string_train = combine_df.clean_tweet[:train_df.shape[0]].to_string()\n",
    "string_test = combine_df.clean_tweet[train_df.shape[0]:].to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = len(string_combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 41s, sys: 21 s, total: 2min 2s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc = nlp(string_combine)\n",
    "doc_train = nlp(string_train)\n",
    "doc_test = nlp(string_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for e in doc.ents:\n",
    "    labels.append((e.label_, e.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in labels:\n",
    "    ner[word[0]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CARDINAL', 41827),\n",
       " ('DATE', 9672),\n",
       " ('PERSON', 5725),\n",
       " ('GPE', 3492),\n",
       " ('ORG', 2272),\n",
       " ('TIME', 1372),\n",
       " ('NORP', 908),\n",
       " ('ORDINAL', 512),\n",
       " ('FAC', 391),\n",
       " ('PRODUCT', 323),\n",
       " ('LOC', 169),\n",
       " ('EVENT', 145),\n",
       " ('QUANTITY', 113),\n",
       " ('LAW', 64),\n",
       " ('WORK_OF_ART', 43),\n",
       " ('PERCENT', 35),\n",
       " ('MONEY', 35),\n",
       " ('LANGUAGE', 25)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных.   Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_train = collections.Counter()\n",
    "person_test = collections.Counter()\n",
    "organization_train = collections.Counter()\n",
    "organization_test = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in doc_train.ents:\n",
    "    if e.label_ == 'ORG':\n",
    "        organization_train[e.text] += 1\n",
    "    elif e.label_ == 'PERSON':\n",
    "        person_train[e.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en in doc_test.ents:\n",
    "    if en.label_ == 'ORG':\n",
    "        organization_test[en.text] += 1\n",
    "    elif en.label_ == 'PERSON':\n",
    "        person_test[en.text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame()\n",
    "dd['popular_organization_train'] = organization_train.most_common(20)\n",
    "dd['popular_organization_test'] = organization_test.most_common(20)\n",
    "dd['popular_person_train'] = person_train.most_common(20)\n",
    "dd['popular_person_test'] = person_test.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popular_organization_train</th>\n",
       "      <th>popular_organization_test</th>\n",
       "      <th>popular_person_train</th>\n",
       "      <th>popular_person_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(gop, 33)</td>\n",
       "      <td>(nba, 10)</td>\n",
       "      <td>(conde, 26)</td>\n",
       "      <td>(obama, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(nba, 22)</td>\n",
       "      <td>(gop, 9)</td>\n",
       "      <td>(hillary, 19)</td>\n",
       "      <td>(feminismisterrorism feminismm, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(cavs, 20)</td>\n",
       "      <td>(islam, 9)</td>\n",
       "      <td>(christina grimmie, 18)</td>\n",
       "      <td>(suppo, 13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(social analytics, 14)</td>\n",
       "      <td>(cavs, 7)</td>\n",
       "      <td>(obama, 17)</td>\n",
       "      <td>(fath, 11)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(un, 13)</td>\n",
       "      <td>(un, 6)</td>\n",
       "      <td>(feminismisterrorism feminismm, 17)</td>\n",
       "      <td>(sta, 9)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(islam, 12)</td>\n",
       "      <td>(starbucks, 6)</td>\n",
       "      <td>(bjp, 17)</td>\n",
       "      <td>(lebron, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(cnn, 11)</td>\n",
       "      <td>(fbi, 5)</td>\n",
       "      <td>(sta, 16)</td>\n",
       "      <td>(jo cox, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(ios, 9)</td>\n",
       "      <td>(sirf ashiq log, 5)</td>\n",
       "      <td>(bea, 14)</td>\n",
       "      <td>(bjp, 8)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(fami, 8)</td>\n",
       "      <td>(congress, 5)</td>\n",
       "      <td>(fath, 13)</td>\n",
       "      <td>(christina grimmie, 7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(fed, 8)</td>\n",
       "      <td>(bbc, 4)</td>\n",
       "      <td>(tom, 12)</td>\n",
       "      <td>(lo, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(allahsoil, 8)</td>\n",
       "      <td>(ht, 4)</td>\n",
       "      <td>(donald trump, 12)</td>\n",
       "      <td>(ali, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(buffalo simulation buffalo, 7)</td>\n",
       "      <td>(ge, 4)</td>\n",
       "      <td>(detoxdiet altwaystoheal, 12)</td>\n",
       "      <td>(kolianwali, 6)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(congress, 7)</td>\n",
       "      <td>(buffalo simulation buffalo, 4)</td>\n",
       "      <td>(wasi f, 11)</td>\n",
       "      <td>(lucy, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(fbi, 7)</td>\n",
       "      <td>(aap, 4)</td>\n",
       "      <td>(jo cox, 11)</td>\n",
       "      <td>(jesus, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(anc, 7)</td>\n",
       "      <td>(ukip, 4)</td>\n",
       "      <td>(lebron, 10)</td>\n",
       "      <td>(ann, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(np, 7)</td>\n",
       "      <td>(wa, 4)</td>\n",
       "      <td>(anton yelchin, 9)</td>\n",
       "      <td>(regrann, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(eu, 6)</td>\n",
       "      <td>(gl, 4)</td>\n",
       "      <td>(karen iqbal, 9)</td>\n",
       "      <td>(detoxdiet altwaystoheal, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(sirf ashiq log, 6)</td>\n",
       "      <td>(lat, 4)</td>\n",
       "      <td>(luv hottweets, 8)</td>\n",
       "      <td>(clinton, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(payintheusa polar, 6)</td>\n",
       "      <td>(pll, 4)</td>\n",
       "      <td>(shi, 8)</td>\n",
       "      <td>(alkalamba showcase, 5)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(olathe polar, 6)</td>\n",
       "      <td>(dem, 4)</td>\n",
       "      <td>(bong bing, 8)</td>\n",
       "      <td>(wasi f, 5)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         popular_organization_train        popular_organization_test  \\\n",
       "0                         (gop, 33)                        (nba, 10)   \n",
       "1                         (nba, 22)                         (gop, 9)   \n",
       "2                        (cavs, 20)                       (islam, 9)   \n",
       "3            (social analytics, 14)                        (cavs, 7)   \n",
       "4                          (un, 13)                          (un, 6)   \n",
       "5                       (islam, 12)                   (starbucks, 6)   \n",
       "6                         (cnn, 11)                         (fbi, 5)   \n",
       "7                          (ios, 9)              (sirf ashiq log, 5)   \n",
       "8                         (fami, 8)                    (congress, 5)   \n",
       "9                          (fed, 8)                         (bbc, 4)   \n",
       "10                   (allahsoil, 8)                          (ht, 4)   \n",
       "11  (buffalo simulation buffalo, 7)                          (ge, 4)   \n",
       "12                    (congress, 7)  (buffalo simulation buffalo, 4)   \n",
       "13                         (fbi, 7)                         (aap, 4)   \n",
       "14                         (anc, 7)                        (ukip, 4)   \n",
       "15                          (np, 7)                          (wa, 4)   \n",
       "16                          (eu, 6)                          (gl, 4)   \n",
       "17              (sirf ashiq log, 6)                         (lat, 4)   \n",
       "18           (payintheusa polar, 6)                         (pll, 4)   \n",
       "19                (olathe polar, 6)                         (dem, 4)   \n",
       "\n",
       "                   popular_person_train                  popular_person_test  \n",
       "0                           (conde, 26)                          (obama, 20)  \n",
       "1                         (hillary, 19)  (feminismisterrorism feminismm, 18)  \n",
       "2               (christina grimmie, 18)                          (suppo, 13)  \n",
       "3                           (obama, 17)                           (fath, 11)  \n",
       "4   (feminismisterrorism feminismm, 17)                             (sta, 9)  \n",
       "5                             (bjp, 17)                          (lebron, 8)  \n",
       "6                             (sta, 16)                          (jo cox, 8)  \n",
       "7                             (bea, 14)                             (bjp, 8)  \n",
       "8                            (fath, 13)               (christina grimmie, 7)  \n",
       "9                             (tom, 12)                              (lo, 6)  \n",
       "10                   (donald trump, 12)                             (ali, 6)  \n",
       "11        (detoxdiet altwaystoheal, 12)                      (kolianwali, 6)  \n",
       "12                         (wasi f, 11)                            (lucy, 5)  \n",
       "13                         (jo cox, 11)                           (jesus, 5)  \n",
       "14                         (lebron, 10)                             (ann, 5)  \n",
       "15                   (anton yelchin, 9)                         (regrann, 5)  \n",
       "16                     (karen iqbal, 9)         (detoxdiet altwaystoheal, 5)  \n",
       "17                   (luv hottweets, 8)                         (clinton, 5)  \n",
       "18                             (shi, 8)              (alkalamba showcase, 5)  \n",
       "19                       (bong bing, 8)                          (wasi f, 5)  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2.  \n",
    "Используя библиотеку nltk, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? Для данного задания используем ограничение на количество символов во входном датасете (max_word_limit_spacy = 1000000), чтобы иметь возможность сравнить результаты работы Spacy и nltk. Обратите внимание, что nltk чувствителен к регистру.  \n",
    "С помощью nltk выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_combine_ = combine_df.tweet.to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_combine_tokenize = nltk.word_tokenize(string_combine_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_combine_tags = nltk.pos_tag(string_combine_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_combine_chunk = nltk.ne_chunk (string_combine_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(string_combine))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никаких сущностей, к сожалению,  nltk не нашел"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitc19fc642da9a416d9fd6990b10e8cd61"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
